import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import mutual_info_classif
from joblib import dump

def get_data():
    from sklearn.datasets import load_breast_cancer
    data = load_breast_cancer(as_frame=True)
    df = pd.concat([data['data'], data['target']], axis=1)
    return df

# Load the Breast Cancer Wisconsin dataset
data = get_data()

# Display the first few rows
data.head()

# Distribution of the target variable
sns.countplot(x=data['target'])
plt.title('Distribution of Target Variable (Malignant vs Benign)')
plt.show()

from sklearn.feature_selection import mutual_info_classif

# Split the data into features and target
X = data.drop("target", axis=1)
y = data["target"]

def Show_Feature_Score(X,y):
    # Compute the mutual information scores
    mi_scores = mutual_info_classif(X, y, random_state=42)

    # Create a DataFrame to display the scores
    mi_scores_df = pd.DataFrame({
        'Feature': X.columns,
        'Mutual Information': mi_scores
    })

    # Sort the DataFrame by mutual information scores
    mi_scores_df = mi_scores_df.sort_values(by='Mutual Information', ascending=False)

    # Visualize the mutual information scores
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Mutual Information', y='Feature', data=mi_scores_df, palette="viridis")
    plt.title('Mutual Information Scores for Each Feature')
    plt.show()

Show_Feature_Score(X,y)

def add_combined_feature(X):
    X = X.copy()  # Ensure we're modifying a copy of the DataFrame
    
    # Example feature: combining two features
    X['Combined_radius_texture'] = X['mean radius'] * X['mean texture']
    
    return X

from sklearn.preprocessing import FunctionTransformer

# Define the feature engineering and preprocessing pipeline
preprocessing_pipeline = Pipeline([
    ('feature_engineering', FunctionTransformer(add_combined_feature)),
    ('scaler', StandardScaler())
])

# Define the models and their hyperparameters for GridSearchCV
models = [
    {
        'classifier': [LogisticRegression(max_iter=1000)],
        'classifier__C': [0.1, 1.0, 10]
    }
]

# Updated pipeline with additional feature engineering and data transformation steps
training_pipeline = Pipeline(steps=[
    ('preprocessing', preprocessing_pipeline),
    ('classifier', LogisticRegression()) # Placeholder, will be replaced by GridSearchCV
])

# Split the data into training and testing sets
X = data.drop(columns=['target'])
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Use GridSearchCV to find the best model and hyperparameters
grid_search = GridSearchCV(training_pipeline, models, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Print the best parameters and the corresponding score
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.2f}")

# Best model
best_model = grid_search.best_estimator_

def evaluate_model(model, X_test, y_test):

    # Make predictions on the test set
    y_pred = model.predict(X_test)
    # Evaluate the model's performance
    print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Evaluate the best Model
evaluate_model(best_model, X_test, y_test)

# Save the best model and the preprocessing steps
dump(best_model, 'best_cancer_model_pipeline.joblib')

# ASSIGNMENT SOLUTION: Create a new pipeline, with a different feature engineering, and different models

def add_advanced_features(X):
    X = X.copy()
    
    X['area_perimeter_ratio'] = X['mean area'] / (X['mean perimeter'] + 1e-10)
    X['concavity_compactness_ratio'] = X['mean concavity'] / (X['mean compactness'] + 1e-10)
    X['radius_squared'] = X['mean radius'] ** 2
    X['area_squared'] = X['mean area'] ** 2
    X['worst_mean_radius_diff'] = X['worst radius'] - X['mean radius']
    X['worst_mean_area_diff'] = X['worst area'] - X['mean area']
    X['texture_stats'] = X[['mean texture', 'worst texture', 'texture error']].mean(axis=1)
    X['smoothness_stats'] = X[['mean smoothness', 'worst smoothness', 'smoothness error']].mean(axis=1)
    X['tumor_severity_index'] = (X['worst radius'] * X['worst concavity'] * X['worst concave points']) / 100
    
    return X

# Define the feature engineering and preprocessing pipeline
new_preprocessing_pipeline = Pipeline([
    ('feature_engineering', FunctionTransformer(add_advanced_features)),
    ('scaler', StandardScaler())
])

# Updated pipeline with additional feature engineering and data transformation steps
new_training_pipeline = Pipeline(steps=[
    ('preprocessing', new_preprocessing_pipeline),
    ('classifier', LogisticRegression()) # Placeholder, will be replaced by GridSearchCV
])

# Define the models and their hyperparameters for GridSearchCV
new_models = [
    {
        'classifier': [RandomForestClassifier(random_state=42)],
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20]
    },
    {
        'classifier': [SVC(random_state=42)],
        'classifier__C': [0.1, 1, 10],
        'classifier__kernel': ['linear', 'rbf']
    }
]

# Use GridSearchCV to find the best model and hyperparameters
grid_search_new = GridSearchCV(new_training_pipeline, new_models, cv=5, n_jobs=-1, verbose=1)
grid_search_new.fit(X_train, y_train)

# Print the best parameters and the corresponding score
print(f"Best parameters found: {grid_search_new.best_params_}")
print(f"Best cross-validation score: {grid_search_new.best_score_:.2f}")

# Best model
best_model_new = grid_search_new.best_estimator_

# Evaluate the best Model
evaluate_model(best_model_new, X_test, y_test)

# Save the best model and the preprocessing steps
dump(best_model_new, 'best_cancer_model_rf_svc_pipeline.joblib')
